{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy import ndimage\n",
    "import scipy.misc\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator , array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.convolutional import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_PATH = os.getcwd()\n",
    "PATH_TO_FILE_STREAM = 'G:\\\\Shared drives\\\\P-Sick'\n",
    "PATH_TO_IMAGES = os.path.join(PATH_TO_FILE_STREAM, 'small\\\\Florence\\\\20180917a_jpgs\\\\jpgs')\n",
    "PATH_TO_TAG_CSV = 'G:\\\\Shared drives\\\\P-Sick\\\\tag_csv\\\\tagging_data.csv'\n",
    "\n",
    "PERCENT_TEST = 0.2\n",
    "PERCENT_TRAINING = (1-PERCENT_TEST)\n",
    "PERCENT_VALIDATION = 0.2\n",
    "PERCENT_VALIDATION = PERCENT_VALIDATION/PERCENT_TRAINING\n",
    "\n",
    "RESIZE_HEIGHT,RESIZE_WIDTH = 224,224\n",
    "\n",
    "TRAINING_IMAGE_FOLDER = \"..\\\\_training_images\"\n",
    "TESTING_IMAGE_FOLDER = \"..\\\\_testing_images\"\n",
    "VALIDATION_IMAGE_FOLDER = \"..\\\\_validation_images\"\n",
    "\n",
    "NUM_TRAINING_BATCHES = 20\n",
    "NUM_TESTING_BATCHES = 20\n",
    "NUM_VALIDATION_BATCHES = 10\n",
    "\n",
    "IMPACT_CLASSES = ['0','1','2','3','4']\n",
    "IMPACT_CLASSES_INT = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "These are a list of the ids for impact\n",
    "```\n",
    "NoneId:0  \n",
    "SwashId:1  \n",
    "CollisionId:2  \n",
    "OverwashId:3  \n",
    "InundationId:4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the image data\n",
    "\n",
    "1. We need to load the data from the csv\n",
    "2. Split the images up into training and test set, and then place them in seperate folders."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First lets load the csv that has all the completely tagged image tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_tags = pd.read_csv(PATH_TO_TAG_CSV)\n",
    "df_impact_images = df_image_tags[['image_id','impact']]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training,testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of images 148 50 50\nBatch sizes 7 2 5\n"
    }
   ],
   "source": [
    "df_training_images, df_testing_images = train_test_split(df_impact_images, test_size=PERCENT_TEST , random_state=1123567) #\n",
    "df_training_images, df_validation_images = train_test_split(df_training_images, test_size=PERCENT_VALIDATION, random_state=67123346) #, random_state=1337\n",
    "\n",
    "NUM_TRAINING_IMAGES = len(df_training_images)\n",
    "NUM_TESTING_IMAGES = len(df_testing_images)\n",
    "NUM_VALIDATION_IMAGES = len(df_validation_images)\n",
    "\n",
    "SIZE_TRAINING_BATCH = NUM_TRAINING_IMAGES//NUM_TRAINING_BATCHES\n",
    "SIZE_TESTING_BATCH = NUM_TESTING_IMAGES//NUM_TESTING_BATCHES\n",
    "SIZE_VALIDATION_BATCH = NUM_VALIDATION_IMAGES//NUM_VALIDATION_BATCHES\n",
    "\n",
    "print('Number of images',NUM_TRAINING_IMAGES,NUM_TESTING_IMAGES,NUM_VALIDATION_IMAGES)\n",
    "\n",
    "print('Batch sizes',SIZE_TRAINING_BATCH,SIZE_TESTING_BATCH,SIZE_VALIDATION_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images into the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Before we copy the images, remove all files within these folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_folders(folders):\n",
    "    for folder in folders:\n",
    "        try:\n",
    "            shutil.rmtree(folder)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_folders([TRAINING_IMAGE_FOLDER,TESTING_IMAGE_FOLDER,VALIDATION_IMAGE_FOLDER])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create the training, test, and validation folders, and in each have a folder where the folders name is the impact type(0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folders(folders):\n",
    "    for folder in folders:\n",
    "        try:\n",
    "            os.mkdir(folder)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            for i in ['1','2','3','4']:#IMPACT_CLASSES:\n",
    "                os.mkdir(os.path.join(folder, f'{i}'))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_folders([TRAINING_IMAGE_FOLDER,TESTING_IMAGE_FOLDER,VALIDATION_IMAGE_FOLDER])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Copy our training, test, and vaildation images into their respective folders, while keeping metadata with copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(dict_of_data_sets):\n",
    "    for sets in dict_of_data_sets:\n",
    "        for index, row in sets['df'].iterrows():\n",
    "            if(not str(row['impact']) == '0'):   \n",
    "                impact_path =  os.path.join(sets['folder_path'],str(row['impact']))\n",
    "                full_new_path =  os.path.join(impact_path,row['image_id'])\n",
    "\n",
    "                meme = shutil.copy2(os.path.join(PATH_TO_IMAGES,row['image_id']),  full_new_path)\n",
    "                \n",
    "                img = load_img(full_new_path) \n",
    "\n",
    "                x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "                x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "                num_augmented_img = 0\n",
    "                max_augmented_images = 8\n",
    "                for batch in gen.flow(x, batch_size=1,\n",
    "                                        save_to_dir=impact_path, save_prefix='augmented', save_format='jpeg'):\n",
    "                    num_augmented_img += 1\n",
    "                    if num_augmented_img > max_augmented_images:\n",
    "                        break  # otherwise the generator would loop indefinitely\n",
    "    \n",
    "    print('done copying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "done copying\n"
    }
   ],
   "source": [
    "copy_images([\n",
    "    {\n",
    "        'folder_path':TRAINING_IMAGE_FOLDER,\n",
    "        'df':df_training_images\n",
    "    },\n",
    "    {\n",
    "        'folder_path':TESTING_IMAGE_FOLDER,\n",
    "        'df':df_testing_images\n",
    "    },\n",
    "     {\n",
    "        'folder_path':VALIDATION_IMAGE_FOLDER,\n",
    "        'df':df_validation_images\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>impact</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>C26051403.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>C26047788.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>P26054595.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>P26051780.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>P26057336.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>C26049720.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>P26049162.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>P26060200.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>C26052087.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>P26058106.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>P26059398.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>C26049539.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>C26047722.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>P26060330.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>C26052596.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>P26056063.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>P26058411.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>P26060284.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>P26060248.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>C26048809.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>P26059726.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>C26050401.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>P26058384.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>C26048971.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>S26047308.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>215</th>\n      <td>C26048134.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>C26048998.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>P26060146.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>P26048301.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>C26047380.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>P26056253.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>C26050997.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>P26048990.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P26050957.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>212</th>\n      <td>P26060077.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>C26048378.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>P26055885.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>P26048319.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>P26050963.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>219</th>\n      <td>P26055517.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>C26051769.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>P26052083.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>P26058185.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>P26056114.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>P26052014.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>P26056262.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>P26049765.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P26057146.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>P26050431.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>P26048352.jpg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "          image_id  impact\n10   C26051403.jpg       0\n229  C26047788.jpg       2\n208  P26054595.jpg       3\n245  P26051780.jpg       0\n85   P26057336.jpg       2\n60   C26049720.jpg       0\n130  P26049162.jpg       0\n101  P26060200.jpg       3\n121  C26052087.jpg       0\n156  P26058106.jpg       3\n12   P26059398.jpg       1\n136  C26049539.jpg       0\n14   C26047722.jpg       0\n235  P26060330.jpg       3\n180  C26052596.jpg       0\n8    P26056063.jpg       3\n161  P26058411.jpg       3\n165  P26060284.jpg       2\n174  P26060248.jpg       2\n222  C26048809.jpg       0\n191  P26059726.jpg       1\n146  C26050401.jpg       0\n204  P26058384.jpg       3\n102  C26048971.jpg       0\n232  S26047308.jpg       0\n215  C26048134.jpg       0\n143  C26048998.jpg       0\n171  P26060146.jpg       1\n209  P26048301.jpg       0\n133  C26047380.jpg       0\n44   P26056253.jpg       2\n114  C26050997.jpg       0\n37   P26048990.jpg       0\n3    P26050957.jpg       0\n212  P26060077.jpg       2\n50   C26048378.jpg       0\n97   P26055885.jpg       2\n77   P26048319.jpg       0\n132  P26050963.jpg       0\n219  P26055517.jpg       3\n239  C26051769.jpg       0\n109  P26052083.jpg       0\n167  P26058185.jpg       3\n64   P26056114.jpg       3\n61   P26052014.jpg       0\n84   P26056262.jpg       2\n42   P26049765.jpg       0\n1    P26057146.jpg       2\n82   P26050431.jpg       0\n145  P26048352.jpg       0"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing_images"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the images\n",
    "\n",
    "1. create NP arrays for Keras by pointing to where the images are for each set, since the images in each set are in folders labeled 0-4 Keras will be able to apply a class to them\n",
    "\n",
    "I also tried to find out how to determine a good batch size, and I found [this](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network) which while not the most scholarly of places, is good enough for me "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = ImageDataGenerator().flow_from_directory(TRAINING_IMAGE_FOLDER, target_size = (RESIZE_HEIGHT,RESIZE_WIDTH), classes = IMPACT_CLASSES, batch_size = SIZE_TRAINING_BATCH)\n",
    "testing_batch = ImageDataGenerator().flow_from_directory(TESTING_IMAGE_FOLDER, target_size = (RESIZE_HEIGHT,RESIZE_WIDTH), classes = IMPACT_CLASSES, batch_size = SIZE_TESTING_BATCH)\n",
    "validation_batch = ImageDataGenerator().flow_from_directory(VALIDATION_IMAGE_FOLDER, target_size = (RESIZE_HEIGHT,RESIZE_WIDTH), classes = IMPACT_CLASSES, batch_size = SIZE_VALIDATION_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This function `plots` is used to just show some of the images in a batch and their class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    \n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(training_batch)\n",
    "plots(imgs,titles=labels,figsize=(24,6),rows = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = tensorflow.keras.applications.vgg16.VGG16()\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for layer in model_vgg16.layers[:-1]:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(len(IMPACT_CLASSES), activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.0001),loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH= 5\n",
    "VALIDATION_STEPS = 5\n",
    "NUM_EPOCH = 5\n",
    "\n",
    "model.fit_generator(training_batch,steps_per_epoch = STEPS_PER_EPOCH, validation_data = validation_batch, validation_steps = VALIDATION_STEPS, epochs = NUM_EPOCH, verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testing_batch,verbose = 2)\n",
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = model.predict_classes(testing_batch,batch_size = None,verbose = 2)\n",
    "rounded_predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred = rounded_predictions,y_true = testing_batch.labels,labels = IMPACT_CLASSES_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax,cmap=sns.diverging_palette(240, 10, n=25)); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(IMPACT_CLASSES_INT); ax.yaxis.set_ticklabels(IMPACT_CLASSES_INT);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}